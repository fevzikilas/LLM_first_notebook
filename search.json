[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fine Tune LLAMA 2",
    "section": "",
    "text": "Step 1: Install All the Required Packages\n\n!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl\n\n\n\nStep 2: Import All the Required Libraries\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n\n\n\nStep 3: Load & Train model\nLlma2 modelini fine tune edebilmek için belli formatta datasetine ihtiyacımız var. Ve format resimde ki gibidir. \nBu Promtta veri setine sahipsek istedeiğimiz şekilde modeli fine tune edebiliriz.\nŞimdi sıradaki adım uygun bir veri seti bulup o veri setini nasil bu formata getirebiliriz onu yapacaz.\nBulduğum uygun bir Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n\nReformat Dataset following the Llama 2 template with 1k sample: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\n\n\nComplete Reformat Dataset following the Llama 2 template: https://huggingface.co/datasets/mlabonne/guanaco-llama2\n\nTo know how this dataset was created, you can check this notebook. https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing\n\n# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model name\nnew_model = \"Llama-2-7b-chat-finetune\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension KIND OF HYPERPARAMETER\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters SUPERVISED TUNING\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n\n\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major &gt;= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nInput In [31], in &lt;cell line: 15&gt;()\n     14 # Check GPU compatibility with bfloat16\n     15 if compute_dtype == torch.float16 and use_4bit:\n---&gt; 16     major, _ = torch.cuda.get_device_capability()\n     17     if major &gt;= 8:\n     18         print(\"=\" * 80)\n\nFile ~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:451, in get_device_capability(device)\n    438 def get_device_capability(device: Optional[_device_t] = None) -&gt; Tuple[int, int]:\n    439     r\"\"\"Get the cuda capability of a device.\n    440 \n    441     Args:\n   (...)\n    449         tuple(int, int): the major and minor cuda capability of the device\n    450     \"\"\"\n--&gt; 451     prop = get_device_properties(device)\n    452     return prop.major, prop.minor\n\nFile ~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:465, in get_device_properties(device)\n    455 def get_device_properties(device: _device_t) -&gt; _CudaDeviceProperties:\n    456     r\"\"\"Get the properties of a device.\n    457 \n    458     Args:\n   (...)\n    463         _CudaDeviceProperties: the properties of the device\n    464     \"\"\"\n--&gt; 465     _lazy_init()  # will define _get_device_properties\n    466     device = _get_device_index(device, optional=True)\n    467     if device &lt; 0 or device &gt;= device_count():\n\nFile ~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:305, in _lazy_init()\n    300     raise RuntimeError(\n    301         \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\n    302         \"multiprocessing, you must use the 'spawn' start method\"\n    303     )\n    304 if not hasattr(torch._C, \"_cuda_getDeviceCount\"):\n--&gt; 305     raise AssertionError(\"Torch not compiled with CUDA enabled\")\n    306 if _cudart is None:\n    307     raise AssertionError(\n    308         \"libcudart functions unavailable. It looks like you have a broken build?\"\n    309     )\n\nAssertionError: Torch not compiled with CUDA enabled\n\n\n\n\n\nStep 4: Load everything and start the fine-tuning process\n\nFirst of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\nThen, we’re configuring bitsandbytes for 4-bit quantization.\nNext, we’re loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\nFinally, we’re loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!\n\n\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major &gt;= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nInput In [8], in &lt;cell line: 2&gt;()\n      1 # Load dataset (you can process it here)\n----&gt; 2 dataset = load_dataset(dataset_name, split=\"train\")\n      4 # Load tokenizer and model with QLoRA configuration\n      5 compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nNameError: name 'load_dataset' is not defined\n\n\n\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)\n\n\n\nStep 5: Check the plots on tensorboard, as follows\n\n\nStep 6: Use the text generation pipeline to ask questions like “What is a large language model?”\n\nNote that I’m formatting the input to match Llama 2 prompt template.\n\n\n# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n\n\n# Empty VRAM\ndel model\ndel pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()\n\nYou can train a Llama 2 model on the entire dataset using mlabonne/guanaco-llama2\n\n\nStep 7: Store New Llama2 Model (Llama-2-7b-chat-finetune)\nHow can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything.\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\nStep 8: Pushing Model to Hugging Face Hub\nOur weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n\n\n!huggingface-cli login\n\nmodel.push_to_hub(\"entbappy/Llama-2-7b-chat-finetune\", check_pr=True)\n\ntokenizer.push_to_hub(\"entbappy/Llama-2-7b-chat-finetune\",check_pr=True)\n\nYou can now use this model for inference by loading it like any other Llama 2 model from the Hub.",
    "crumbs": [
      "Fine Tune LLAMA 2"
    ]
  }
]